{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Movie Recommendation System\n",
    "#### This script implements a hybrid recommender system using both content-based filtering (TF-IDF + Naive Bayes) and collaborative filtering (user-item matrix + cosine similarity).\n",
    "\n",
    "\n",
    "\n",
    "### Imports\n",
    "Core Python libraries, data processing, ML tools, and NLP modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import ast\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Setup\n",
    "Download and prepare stopwords and the Porter stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "Configure logging to output both to file and console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        filename='training.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "    # create and configure a console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # adding console handler to the root logger\n",
    "    logging.getLogger().addHandler(console_handler)\n",
    "    logging.info(\"Logging initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Connection\n",
    "Connects to the SQLite database containing ratings and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(db_path='movies.db'):\n",
    " \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        logging.info(\"Connected to SQLite database.\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        logging.error(f\"Database connection failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load ratings and movie metadata from the database or cached CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(conn):\n",
    "\n",
    "    if os.path.exists(\"ratings_pre.csv\") and os.path.exists(\"items_post.csv\"):\n",
    "        ratings = pd.read_csv(\"ratings.csv\")\n",
    "        items = pd.read_csv(\"items_post.csv\")\n",
    "        logging.info(\"Data loaded from CSV files.\")\n",
    "        return ratings, items\n",
    "    \n",
    "    tqdm.pandas()\n",
    "\n",
    "    # loading ratings data\n",
    "    ratings_query = \"\"\"\n",
    "    SELECT userId, movieId, rating FROM ratings_small\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ratings = pd.read_sql(ratings_query, conn)\n",
    "    ratings.to_csv(\"ratings_pre.csv\", index=False, sep=',')\n",
    "\n",
    "    logging.info(f\"Ratings loaded: {ratings.shape[0]} rows.\")\n",
    "\n",
    "    # loading movie metadata\n",
    "    movies_query = \"\"\"\n",
    "    SELECT m.id, m.title, m.overview, m.genres, c.cast, c.crew, k.keywords\n",
    "    FROM movies_metadata m\n",
    "    INNER JOIN credits c ON c.id = m.id\n",
    "    INNER JOIN keywords k ON k.id = m.id\n",
    "    \"\"\"\n",
    "    items = pd.read_sql(movies_query, conn)\n",
    "\n",
    "    items.to_csv(\"items_pre.csv\", index=False, sep=',')\n",
    "\n",
    "    # turning JSON like columns into strings\n",
    "    items['genres'] = items['genres'].apply(\n",
    "        lambda x: \" \".join(sorted(genre['name'] for genre in ast.literal_eval(x))) if pd.notnull(x) else \"\"\n",
    "    )\n",
    "\n",
    "    # for cast: removing internal spaces from names, sort, and join\n",
    "    items['cast'] = items['cast'].apply(\n",
    "        lambda x: \" \".join(\n",
    "            sorted(member[\"name\"].replace(\" \", \"\") for member in ast.literal_eval(x))\n",
    "        ) if pd.notnull(x) else \"\"\n",
    "    )\n",
    "    \n",
    "    # for crew: filter for Director, Screenplay, and Original Story, removing internal spaces from names, sort, and join\n",
    "    items['crew'] = items['crew'].apply(\n",
    "        lambda x: \" \".join(\n",
    "            sorted(member[\"name\"].replace(\" \", \"\") for member in ast.literal_eval(x)\n",
    "                   if member[\"job\"] in [\"Director\", \"Screenplay\", \"Original Story\"])\n",
    "        ) if pd.notnull(x) else \"\"\n",
    "    )\n",
    "   \n",
    "    # for keywords: extract  'name' from each dict and join with a space\n",
    "    items['keywords'] = items['keywords'].apply(\n",
    "        lambda x: \" \".join(d['name'] for d in ast.literal_eval(x)) if pd.notnull(x) else \"\"\n",
    "    )\n",
    "    items.to_csv(\"items_post.csv\", index=False, sep=',')\n",
    "    \n",
    "    logging.info(f\"Movie metadata loaded: {items.shape[0]} rows.\")\n",
    "    return ratings, items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "Basic NLP preprocessing: lowercasing, stemming, stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):    \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tags\n",
    "Merge external user tags with the items dataframe, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tags(items, path=\"tags.csv\"):\n",
    "    if os.path.exists(path):\n",
    "        tags_df = pd.read_csv(path)\n",
    "        tags_df = tags_df[['movieId', 'tag']].dropna()\n",
    "        tags_agg = tags_df.groupby('movieId')['tag'].apply(lambda x: ' '.join(str(t) for t in x)).reset_index()\n",
    "        logging.info(f\"Loaded and aggregated tags: {tags_agg.shape[0]} movies with tags.\")\n",
    "\n",
    "        items = items.merge(tags_agg, on ='movieId', how='left')\n",
    "        items['tag'] = items['tag'].fillna('')\n",
    "    else:\n",
    "        logging.warning(f\"{path} not found. Skipping tag integration.\")\n",
    "        items['tag'] = ''\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Combines relevant text fields and applies TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(items, min_df_threshold=2): \n",
    "    # filling NaN values\n",
    "    items.fillna(' ', inplace=True)\n",
    "\n",
    "    # clean combined text\n",
    "    items['clean_text'] = items['overview'].apply(text_cleaning)\n",
    "\n",
    "    # combining text fields\n",
    "    text_fields = ['title', 'overview', 'genres', 'cast', 'crew', 'keywords', 'tag'] #mc\n",
    "    items['combined_text'] = items[text_fields].astype(str).agg(' '.join, axis=1) #mc\n",
    "\n",
    "    # vectorizing cleaned text using TF-IDF\n",
    "    tfidf = TfidfVectorizer(min_df=min_df_threshold, max_features=50000)\n",
    "    item_features = tfidf.fit_transform(items['combined_text'])\n",
    "    logging.info(\"Text data preprocessed and vectorized using TF-IDF.\")\n",
    "    return items, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Creation\n",
    "Create labels by taking the most common rating for each movie. Used as the target variable for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(ratings):\n",
    "    item_labels = ratings.groupby('movieId')['rating'].apply(\n",
    "        lambda x: x.mode()[0] if not x.mode().empty else 0\n",
    "    ).reset_index(name='label')\n",
    "    logging.info(\"Labels created from ratings.\")\n",
    "    return item_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Labels\n",
    "Merges labels with the item (movie) metadata dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_labels(items, labels_df): \n",
    "\n",
    "    items['id'] = items['id'].astype(int)\n",
    "    items['movieId'] = items['id'] \n",
    "\n",
    "    items = items.merge(labels_df, left_on ='id', right_on='movieId', how='left')\n",
    "    items['label'] = items['label'].fillna(0).astype(int)\n",
    "\n",
    "    items.drop(columns=['movieId_y'], inplace=True, errors='ignore') \n",
    "    items.rename(columns= {'movieId_x': 'movieId'}, inplace=True) \n",
    "\n",
    "    logging.info(\"Labels merged with movie metadata.\")\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Naive Bayes\n",
    " Tests different values for 'alpha' and 'fit_prior' using 5-fold CV. Selects the best model based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(item_features, labels):\n",
    "    param_grid = {\n",
    "        'alpha': [0.1, 0.25, 0.5 ],\n",
    "        'fit_prior': [True, False]\n",
    "    }\n",
    "    grid = GridSearchCV(MultinomialNB(), param_grid, cv = 5, scoring ='accuracy')\n",
    "    grid.fit(item_features, labels)\n",
    "    logging.info(f\"Best parameters: {grid.best_params_}\")\n",
    "    logging.info(f\"Best CV score: {grid.best_score_:.4f}\")\n",
    "    print(f\"Naive Bayes best parameters: {grid.best_params_}\")\n",
    "    print(f\"Naive Bayes best CV score: {grid.best_score_:.4f}\")\n",
    "\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Rating Matrix\n",
    "Builds a user-item matrix for collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rating_matrix(ratings):\n",
    "    rating_matrix = ratings.pivot(index ='userId', columns ='movieId', values ='rating').fillna(0)\n",
    "    logging.info(\"Rating matrix created for collaborative filtering.\")\n",
    "    return rating_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Ratings\n",
    "Normalizes ratings by subtracting each user's mean rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ratings(rating_matrix): \n",
    "    user_means = rating_matrix.mean(axis=1)\n",
    "    norm_ratings = rating_matrix.subtract(user_means, axis=0)\n",
    "    logging.info(\"User ratings normalized.\")\n",
    "    return norm_ratings, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Item Similarity\n",
    "Calculates cosine similarity between items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_similarity(norm_ratings):\n",
    "\n",
    "    item_sim = cosine_similarity(norm_ratings.T)\n",
    "    logging.info(\"Item similarity matrix computed. \")\n",
    "    return item_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie ID to Index Mapping\n",
    "Maps movieId to its corresponding column index in the rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_mapping(rating_matrix): \n",
    "\n",
    "    movie_id_to_idx = {movie_id: idx for idx, movie_id in enumerate(rating_matrix.columns)}\n",
    "    logging.info(\"Movie ID to index mapping created.\")\n",
    "    return movie_id_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Prediction\n",
    "CF uses user-item similarity and normalized ratings. NB uses text features and a trained classifier. Final prediction is chosen based on:\n",
    "- NB confidence (difference between top predicted class probabilities)\n",
    "- Similarity between NB and CF predictions\n",
    "- Fallbacks when one method lacks enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, target_movie_id, rating_matrix, user_means, item_sim,\n",
    "                   movie_id_to_idx, items, item_features, nb, alpha=0.25, beta=0.4):\n",
    "    cf_pred = 0.0  # collaborative filtering prediction (default to 0)\n",
    "\n",
    "    # --- Collaborative Filtering (CF) Prediction ---\n",
    "    # Check if we have enough data for this user and movie\n",
    "    if target_movie_id in movie_id_to_idx and user_id in rating_matrix.index:\n",
    "        user_ratings = rating_matrix.loc[user_id]         \n",
    "        user_mean = user_means.loc[user_id]                \n",
    "        target_idx = movie_id_to_idx[target_movie_id]      \n",
    "\n",
    "        # get movies this user has rated\n",
    "        rated_movie_ids = user_ratings[user_ratings != 0].index\n",
    "        if not rated_movie_ids.empty:\n",
    "            rated_indices = [movie_id_to_idx[m_id] for m_id in rated_movie_ids if m_id in movie_id_to_idx]\n",
    "            sim_scores = item_sim[target_idx][rated_indices]        # similarity between target and rated movies\n",
    "            user_rated_ratings = user_ratings[rated_movie_ids].values\n",
    "            user_norm_ratings = user_rated_ratings - user_mean      # normalize ratings\n",
    "\n",
    "            # adjust similarity scores based on how many users rated both items\n",
    "            min_common = 50\n",
    "            for i, rated_id in enumerate(rated_movie_ids):\n",
    "                if rated_id not in movie_id_to_idx:\n",
    "                    continue\n",
    "                co_rated = ((rating_matrix[target_movie_id] != 0) & (rating_matrix[rated_id] != 0)).sum()\n",
    "                weight_factor = min(1.0, co_rated / min_common)\n",
    "                sim_scores[i] *= weight_factor  # reduce weight if little overlap\n",
    "\n",
    "            # Compute weighted average prediction\n",
    "            numerator = np.dot(sim_scores, user_norm_ratings)\n",
    "            denominator = np.sum(np.abs(sim_scores))\n",
    "            if denominator > 0:\n",
    "                cf_pred = user_mean + (numerator / denominator)\n",
    "\n",
    "    # --- Content-Based Prediction (Naive Bayes) ---\n",
    "    # Use the trained classifier to predict the rating class from movie text features\n",
    "    item_row = items[items['movieId'] == target_movie_id]\n",
    "    if not item_row.empty:\n",
    "        item_idx = item_row.index[0]\n",
    "        features = item_features[item_idx]\n",
    "        nb_pred = nb.predict(features)[0]              # predicted class\n",
    "        probas = nb.predict_proba(features)[0]         # class probabilities\n",
    "        sorted_probs = np.sort(probas)\n",
    "\n",
    "        # Confidence = difference between top two predicted class probabilities\n",
    "        if len(sorted_probs) > 1:\n",
    "            nb_confidence = sorted_probs[-1] - sorted_probs[-2]\n",
    "        else:\n",
    "            nb_confidence = 0\n",
    "    else:\n",
    "        # Fallback if movie metadata is missing\n",
    "        nb_pred = user_means.mean()\n",
    "        nb_confidence = 0\n",
    "\n",
    "    # --- Hybrid Decision Logic ---\n",
    "    # Decide whether to trust CF or NB, or blend them\n",
    "\n",
    "    if cf_pred == 0:\n",
    "        # CF has no prediction (user/movie not known), fall back to NB\n",
    "        pred = nb_pred\n",
    "\n",
    "    elif nb_confidence > alpha:\n",
    "        # NB is confident, use its prediction\n",
    "        pred = nb_pred\n",
    "\n",
    "    elif abs(nb_pred - cf_pred) < beta:\n",
    "        # NB and CF predictions are close, use NB\n",
    "        pred = nb_pred\n",
    "\n",
    "    else:\n",
    "        # Tie-breaking logic: check if any of the top NB classes are close to CF prediction\n",
    "        top_prob = sorted_probs[-1]\n",
    "        top_classes = [i for i, p in enumerate(probas) if np.isclose(p, top_prob)]\n",
    "\n",
    "        tie_resolved = False\n",
    "        for tied_class in top_classes:\n",
    "            if abs(tied_class - cf_pred) < beta:\n",
    "                pred = tied_class\n",
    "                tie_resolved = True\n",
    "                break\n",
    "\n",
    "        # If still not resolved, use CF prediction\n",
    "        if not tie_resolved:\n",
    "            pred = cf_pred\n",
    "\n",
    "    # Ensure prediction is in valid rating range\n",
    "    return np.clip(pred, 0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Computes MAE, ROC AUC, and coverage for the hybrid recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ratings, rating_matrix, user_means, item_sim,\n",
    "                   movie_id_to_idx, items, item_features, nb):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    binary_true = []  # 1 if rating > user average, else 0\n",
    "    scores = []       # predicted scores used for ROC AUC\n",
    "\n",
    "    logging.info(\"Starting model evaluation.\")\n",
    "    \n",
    "    # goes through each user-movie-rating in the dataset\n",
    "    for _, row in tqdm(ratings.iterrows(), total=len(ratings), desc=\"Evaluating\"):\n",
    "        user = row['userId']\n",
    "        movie = row['movieId']\n",
    "        true_rating = row['rating']\n",
    "\n",
    "        # gets prediction from hybrid model\n",
    "        pred = predict_rating(user, movie, rating_matrix, user_means, item_sim,\n",
    "                              movie_id_to_idx, items, item_features, nb)\n",
    "\n",
    "        predictions.append(pred)\n",
    "        true_values.append(true_rating)\n",
    "\n",
    "        # creates binary label: 1 if rating above user's average\n",
    "        user_avg = user_means[user]\n",
    "        binary_true.append(1 if true_rating > user_avg else 0)\n",
    "\n",
    "        scores.append(pred)\n",
    "\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "\n",
    "    #  ROC AUC for binary classification\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(binary_true, scores)\n",
    "    except Exception as e:\n",
    "        roc_auc = None\n",
    "        logging.error(f\"ROC AUC calculation failed: {e}\")\n",
    "\n",
    "    # Compute coverage: % of movies that received at least one prediction\n",
    "    predicted_movies = set(ratings['movieId'])\n",
    "    total_movies = set(items['movieId'])\n",
    "    coverage = len(predicted_movies) / len(total_movies) if total_movies else 0\n",
    "\n",
    "    logging.info(\"Predictions generated.\")\n",
    "    logging.info(f\"Hybrid Model MAE: {mae:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        logging.info(f\"Hybrid Model ROC AUC: {roc_auc:.4f}\")\n",
    "    logging.info(f\"Coverage (movies predicted): {coverage:.4f}\")\n",
    "\n",
    "    print(f\"Hybrid Model MAE: {mae:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"Hybrid Model ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Coverage (movies predicted): {coverage:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Coordinates all steps: data loading, preprocessing, training, evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # connecting to the database and load data\n",
    "    try:\n",
    "        conn = connect_db()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database not reached, try csv backup. Error: {e}\")\n",
    "\n",
    "    ratings, items = load_data(conn)\n",
    "\n",
    "    \n",
    "    items['movieId'] = items['id']\n",
    "    items = add_tags(items)  \n",
    "\n",
    "    # preprocessing and computing TF-IDF features\n",
    "    items, item_features = preprocess_text(items)\n",
    "\n",
    "    # merging labels with movie metadata\n",
    "    labels_df = create_labels(ratings)\n",
    "    items = merge_labels(items, labels_df)\n",
    "\n",
    "    items['label'] = items['label'].apply(lambda x: 1 if x >= 3 else 0) #mc\n",
    "    labeled_items = items[items['label'] > 0] #mc\n",
    "    labels = labeled_items['label'] #mc\n",
    "    labeled_items = labeled_items.reset_index(drop=True) #mc\n",
    "    item_features = item_features[labeled_items.index] #mc\n",
    "    ratings = ratings[ratings['movieId'].isin(labeled_items['movieId'])] #mc\n",
    "\n",
    "    # tuning via grid search\n",
    "    nb = grid_search(item_features, labels)\n",
    "    logging.info(\"Naive Bayes classifier retrained with optimal hyperparameters.\")\n",
    "\n",
    "    # components for collaborative filtering\n",
    "    rating_matrix = create_rating_matrix(ratings)\n",
    "    norm_ratings, user_means = normalize_ratings(rating_matrix)\n",
    "    item_sim = item_similarity(norm_ratings)\n",
    "    movie_id_to_idx =id_mapping(rating_matrix)\n",
    "\n",
    "    # evaluation\n",
    "    evaluate_model(ratings, rating_matrix, user_means, item_sim,\n",
    "               movie_id_to_idx, labeled_items, item_features, nb) #mc\n",
    "\n",
    "\n",
    "    conn.close()\n",
    "    logging.info(\"Database connection closed.\")\n",
    "    logging.info(\"Training process finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
